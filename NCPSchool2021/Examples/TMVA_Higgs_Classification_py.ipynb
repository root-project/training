{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tmva_logo.gif\" height=\"20%\" width=\"20%\">\n",
    "\n",
    "# TMVA Higgs Classification Example in Python\n",
    "\n",
    "In this example we will still do Higgs classification but we will use together with the native TMVA methods also methods from Keras and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "from ROOT import TMVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Factory\n",
    "\n",
    "Create the Factory class. Later you can choose the methods\n",
    "whose performance you'd like to investigate. \n",
    "\n",
    "The factory is the major TMVA object you have to interact with. Here is the list of parameters you need to pass\n",
    "\n",
    " - The first argument is the base of the name of all the output\n",
    "weightfiles in the directory weight/ that will be created with the \n",
    "method parameters \n",
    "\n",
    " - The second argument is the output file for the training results\n",
    "  \n",
    " - The third argument is a string option defining some general configuration for the TMVA session. For example all TMVA output can be suppressed by removing the \"!\" (not) in front of the \"Silent\" argument in the option string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.TMVA.Tools.Instance()\n",
    "## For PYMVA methods\n",
    "TMVA.PyMethodBase.PyInitialize();\n",
    "\n",
    "\n",
    "outputFile = ROOT.TFile.Open(\"Higgs_ClassificationOutput.root\", \"RECREATE\")\n",
    "\n",
    "factory = ROOT.TMVA.Factory(\"TMVA_Higgs_Classification\", outputFile,\n",
    "                      \"!V:ROC:!Silent:Color:!DrawProgressBar:AnalysisType=Classification\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data\n",
    "\n",
    "We define now the input data file and we retrieve the ROOT TTree objects with the signal and background input events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFileName = \"Higgs_data.root\"\n",
    "\n",
    "inputFile = ROOT.TFile.Open( inputFileName )\n",
    "\n",
    "# retrieve input trees\n",
    "\n",
    "signalTree     = inputFile.Get(\"sig_tree\")\n",
    "backgroundTree = inputFile.Get(\"bkg_tree\")\n",
    "\n",
    "signalTree.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare DataLoader(s)\n",
    "\n",
    "The next step is to declare the DataLoader class that deals with input data abd variables \n",
    "\n",
    "We add first the signal and background trees in the data loader and then we\n",
    "define the input variables that shall be used for the MVA training\n",
    "note that you may also use variable expressions, which can be parsed by TTree::Draw( \"expression\" )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ROOT.TMVA.DataLoader(\"dataset\")\n",
    "\n",
    "### global event weights per tree (see below for setting event-wise weights)\n",
    "signalWeight     = 1.0\n",
    "backgroundWeight = 1.0\n",
    "   \n",
    "### You can add an arbitrary number of signal or background trees\n",
    "loader.AddSignalTree    ( signalTree,     signalWeight     )\n",
    "loader.AddBackgroundTree( backgroundTree, backgroundWeight )\n",
    "\n",
    "## Define input variables \n",
    "\n",
    "loader.AddVariable(\"m_jj\")\n",
    "loader.AddVariable(\"m_jjj\")\n",
    "loader.AddVariable(\"m_lv\")\n",
    "loader.AddVariable(\"m_jlv\")\n",
    "loader.AddVariable(\"m_bb\")\n",
    "loader.AddVariable(\"m_wbb\")\n",
    "loader.AddVariable(\"m_wwbb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Dataset(s)\n",
    "\n",
    "Setup the DataLoader by splitting events in training and test samples. \n",
    "Here we use a random split and a fixed number of training and test events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Apply additional cuts on the signal and background samples (can be different)\n",
    "mycuts = ROOT.TCut(\"\")   ## for example: TCut mycuts = \"abs(var1)<0.5 && abs(var2-0.5)<1\";\n",
    "mycutb = ROOT.TCut(\"\")   ## for example: TCut mycutb = \"abs(var1)<0.5\";\n",
    "\n",
    "\n",
    "loader.PrepareTrainingAndTestTree( mycuts, mycutb,\n",
    "                                  \"nTrain_Signal=1000:nTrain_Background=1000:\"\n",
    "                                  \"nTest_Signal=1000:nTest_Background=1000:\"\n",
    "                                  \"SplitMode=Random:NormMode=NumEvents:!V\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking Methods\n",
    "\n",
    "Here we book the TMVA methods. We book a Likelihood based a BDT and a standard MLP (shallow NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boosted Decision Trees\n",
    "factory.BookMethod(loader,ROOT.TMVA.Types.kBDT, \"BDT\",\n",
    "                   \"!V:NTrees=200:MinNodeSize=2.5%:MaxDepth=2:BoostType=AdaBoost:AdaBoostBeta=0.5:UseBaggedBoost:\"\n",
    "                   \"BaggedSampleFraction=0.5:SeparationType=GiniIndex:nCuts=20\" )\n",
    "\n",
    "## Multi-Layer Perceptron (Neural Network)\n",
    "factory.BookMethod(loader, ROOT.TMVA.Types.kMLP, \"MLP\",\n",
    "                   \"!H:!V:NeuronType=tanh:VarTransform=N:NCycles=100:HiddenLayers=N+5:TestRate=5:!UseRegulator\" );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn\n",
    "\n",
    "here we book some scikit learn packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#factory.BookMethod(loader, ROOT.TMVA.Types.kPyGTB, \"PyGTB\",\"H:!V:VarTransform=G:NEstimators=400:LearningRate=0.1:\"\n",
    "#                                                  \"MaxDepth=3\")\n",
    "#\n",
    "#factory.BookMethod(loader, ROOT.TMVA.Types.kPyRandomForest, \"PyRandomForest\",\"!V:VarTransform=G:NEstimators=400:\"\n",
    "#                           \"Criterion=gini:MaxFeatures=auto:MaxDepth=6:MinSamplesLeaf=3:MinWeightFractionLeaf=0:\"\n",
    "#                            \"Bootstrap=kTRUE\" )\n",
    "#      \n",
    "#factory.BookMethod(loader, ROOT.TMVA.Types.kPyAdaBoost, \"PyAdaBoost\",\"!V:VarTransform=G:NEstimators=400\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Booking Deep Neural Network\n",
    "\n",
    "Here we book the new DNN of TMVA. We use the new DL method available in TMVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define DNN layout\n",
    "\n",
    "we need to define (note the use of the character | as  separator of  input parameters) \n",
    "\n",
    "- input layout :   this defines the input data format for the DNN as  input depth | height | width. \n",
    "   In case of a dense layer as first layer the input layout should be  1 | 1 | number of input variables (features)\n",
    "- batch layout  : this defines how are the input batch. It is related to input layout but not the same. \n",
    "   If the first layer is dense it should be 1 | batch size ! number of variables (fetures)\n",
    "                 \n",
    "- layout string defining the architecture. The syntax is  \n",
    "   - layer type (e.g. DENSE, CONV, RNN)\n",
    "   - layer parameters (e.g. number of units)\n",
    "   - activation function (e.g  TANH, RELU,...)\n",
    "   \n",
    "     the different layers are separated by the \",\" \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLayoutString = \"InputLayout=1|1|7\"; \n",
    "batchLayoutString= \"BatchLayout=1|100|7\";\n",
    "layoutString = (\"Layout=DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|64|TANH,DENSE|1|LINEAR\")                                                                                                                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define Trainining Strategy\n",
    "\n",
    "We define here the different training strategy for the DNN. One can concatenate different training strategy changing parameters like: \n",
    " - Optimizer\n",
    " - Learning rate\n",
    " - Momentum (valid for SGD and RMSPROP)\n",
    " - Regularization and Weight Decay \n",
    " - Dropout \n",
    " - Max number of epochs \n",
    " - Convergence steps. if the test error will not decrease after that value the training will stop\n",
    " - Batch size (This value must be the same specified in the input layout)\n",
    " - Test Repetitions (the interval when the test error will be computed) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training strategies \n",
    "## one can catenate several training strategies\n",
    "\n",
    "training1  = \"Optimizer=ADAM,LearningRate=1e-3,Momentum=0.,Regularization=None,WeightDecay=1e-4,\"\n",
    "training1 += \"DropConfig=0.+0.+0.+0.,MaxEpochs=20,ConvergenceSteps=10,BatchSize=100,TestRepetitions=1\"\n",
    " \n",
    "# we add regularization in the second phase\n",
    "training2  = \"Optimizer=ADAM,LearningRate=1e-3,Momentum=0.,Regularization=L2,WeightDecay=1e-4,\"\n",
    "training2 += \"DropConfig=0.0+0.0+0.0+0,MaxEpochs=20,ConvergenceSteps=10,BatchSize=100,TestRepetitions=1\"\n",
    "     \n",
    "            \n",
    "\n",
    "trainingStrategyString = \"TrainingStrategy=\" + training1 ## + training2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define general options and book method\n",
    "\n",
    "We define the general DNN options such as \n",
    "\n",
    "- Type of Loss function (e.g. cross entropy)\n",
    "- Weight Initizalization (e.g XAVIER, XAVIERUNIFORM, NORMAL )\n",
    "- Variable Transformation\n",
    "- Type of Architecture (e.g. CPU, GPU, Standard)\n",
    "\n",
    "We add then also all the other options defined before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Options.                                                                                                                                                                \n",
    "dnnOptions = \"!H:V:ErrorStrategy=CROSSENTROPY:WeightInitialization=XAVIER::Architecture=CPU\"\n",
    "\n",
    "dnnOptions +=  \":\" + inputLayoutString\n",
    "dnnOptions +=  \":\" + batchLayoutString\n",
    "dnnOptions +=  \":\" + layoutString\n",
    "dnnOptions +=  \":\" + trainingStrategyString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can now book the method\n",
    "              \n",
    "factory.BookMethod(loader, ROOT.TMVA.Types.kDL, \"DL_CPU\", dnnOptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to use tensorflow backend\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Reshape, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='tanh', input_dim=7))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='tanh'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(64, kernel_initializer='glorot_normal', activation='tanh'))\n",
    "model.add(Dense(2, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "# Set loss and optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['categorical_accuracy',])\n",
    "\n",
    "# Store model to file\n",
    "model.save('model_dense.h5')\n",
    "\n",
    "# Print summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory.BookMethod(loader, ROOT.TMVA.Types.kPyKeras, 'Keras_Dense',\n",
    "        'H:!V:FilenameModel=model_dense.h5:'+\\\n",
    "        'NumEpochs=20:BatchSize=100:TriesEarlyStopping=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory.TrainAllMethods();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test  all methods\n",
    "\n",
    "Here we test all methods using the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory.TestAllMethods();   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate all methods\n",
    "\n",
    "Here we evaluate all methods and compare their performances, computing efficiencies, ROC curves etc.. using both training and tetsing data sets. Several histograms are produced which can be examined with the TMVAGui or directly using the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory.EvaluateAllMethods();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC Curve\n",
    "We enable JavaScript visualisation for the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%jsroot on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = factory.GetROCCurve(loader);\n",
    "c1.Draw();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Close outputfile to save all output information (evaluation result of methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##outputFile.Close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
